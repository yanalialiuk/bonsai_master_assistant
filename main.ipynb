{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader\n",
    "import pdfplumber\n",
    "from docx import Document\n",
    "from LLM_caller import APICaller\n",
    "import tiktoken \n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENAIKEY\")\n",
    "api_base = os.getenv(\"ENDPOINT\")\n",
    "api_caller = APICaller(api_key=api_key, endpoint=api_base)\n",
    "client = OpenAI(api_key=api_key, base_url=api_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        reader = PdfReader(pdf_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"ошибка PDF: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_docx(docx_path):\n",
    "    try:\n",
    "        doc = Document(docx_path)\n",
    "        text = \"\\n\".join([paragraph.text for paragraph in doc.paragraphs])\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"ошибка DOCX: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_from_txt(txt_path):\n",
    "    try:\n",
    "        with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"ошибка  TXT: {e}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text, model=\"cl100k_base\"):\n",
    "    try:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")  \n",
    "        return len(encoding.encode(text))  \n",
    "    except Exception as e:\n",
    "        print(f\"ошибка токенов: {e}\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "    try:\n",
    "        token_count = count_tokens(text, model)\n",
    "        max_tokens = 8192  \n",
    "        if token_count > max_tokens:\n",
    "            print(f\"tokens: {token_count}. Max: {max_tokens}.\")\n",
    "            return None\n",
    "        \n",
    "        response = client.embeddings.create(input=text, model=model)\n",
    "        return response.data[0].embedding \n",
    "    except Exception as e:\n",
    "        print(f\"ошибка эмбеддинга: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(embeddings, vector_dim):\n",
    "    try:\n",
    "        index = faiss.IndexFlatL2(vector_dim) \n",
    "        index.add(np.array(embeddings, dtype='float32'))\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        print(f\"ошибка Faiss индекса: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text, chunk_size=8000):\n",
    "\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return []\n",
    "    \n",
    "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents(file_paths):\n",
    "    embeddings = []\n",
    "    documents = []  \n",
    "    for file_path in file_paths:\n",
    "        if file_path.endswith('.pdf'):\n",
    "            text = extract_text_from_pdf(file_path)\n",
    "        elif file_path.endswith('.docx'):\n",
    "            text = extract_text_from_docx(file_path)\n",
    "        elif file_path.endswith('.txt'):\n",
    "            text = extract_text_from_txt(file_path)\n",
    "\n",
    "        chunks = split_text_into_chunks(text, chunk_size=3000)\n",
    "        for chunk in chunks:\n",
    "            embedding = get_embedding(chunk)\n",
    "            if embedding is not None:\n",
    "                embeddings.append(embedding)\n",
    "                documents.append(chunk)\n",
    "\n",
    "    return embeddings, documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response_with_context(query, relevant_texts, api_caller, max_tokens=200, temperature=0.7):\n",
    "    try:\n",
    "        system_message = (\n",
    "            \"You are a bonsai master assistant. There are some relevant texts:\\n\"\n",
    "            + \"\\n\".join([f\"Document {i + 1}: {text}\" for i, text in enumerate(relevant_texts)])\n",
    "            + \"\\nBased on this information answer the question:\"\n",
    "        )\n",
    "        \n",
    "        response = api_caller.call_llm(\n",
    "            system_prompt=system_message,\n",
    "            user_prompt=query,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature\n",
    "        )\n",
    "        \n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"ошибка при генерации ответа {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths = [os.getenv(\"FILESPATHS\")]\n",
    "embeddings, documents = process_documents(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уход за сакурой требует внимания к нескольким важным аспектам:\n",
      "\n",
      "1. **Почва и удобрения**: Сакуре нужна почва, обогащённая гумусом, калием и азотом. Это помогает растению расти здоровым и крепким.\n",
      "\n",
      "2. **Полив**: В сезон сакуру поливают половиной стакана в сутки, а зимой — реже. Важно поддерживать умеренную влажность почвы.\n",
      "\n",
      "3. **Освещение**: Растению необходимо хорошее освещение, но следует избегать сквозняков.\n",
      "\n",
      "4. **Формирование кроны**: Формировать крону можно начиная с 2-3-летнего возраста растения. В зависимости от желаемого стиля, крона может быть прямой, с изогнутыми ветвями или с широко раскинутыми ветвями.\n",
      "\n",
      "5. **Обрезка**: Для миниат\n"
     ]
    }
   ],
   "source": [
    "vector_dim = len(embeddings[0])\n",
    "\n",
    "faiss_index = create_faiss_index(embeddings, vector_dim)\n",
    "if faiss_index is not None:\n",
    "    faiss.write_index(faiss_index, \"vector_database.index\")\n",
    "    \n",
    "    faiss_index = faiss.read_index(\"vector_database.index\")\n",
    "    \n",
    "    query = \"как ухаживать за сакурой?\"\n",
    "    query_embedding = np.array([get_embedding(query)], dtype='float32')\n",
    "    \n",
    "    D, I = faiss_index.search(query_embedding, k=5)  \n",
    "    relevant_texts = [documents[idx] for idx in I[0] if idx != -1]\n",
    "\n",
    "if relevant_texts:\n",
    "    response = generate_response_with_context(query, relevant_texts, api_caller)\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"по запросу ничего не найдено.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
